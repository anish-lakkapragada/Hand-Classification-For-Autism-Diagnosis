{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Let's check how the ensemble does with all and six landmarks. "
   ],
   "metadata": {
    "id": "162d91e9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# first get all X and y data from the all_points_folds\n",
    "import numpy as np \n",
    "import pickle, random \n",
    "import cv2, os\n",
    "\n",
    "X_all, y = [], []  # X needs to be changed for each fold, y doesn't need to be changed for each fold. \n",
    "\n",
    "for file in os.listdir(\"all_points_folds\"): \n",
    "    with open(f\"all_points_folds/{file}\", 'rb') as f: \n",
    "        X_y = pickle.load(f)\n",
    "        X_all.append(X_y[0])\n",
    "        y.append(X_y[1])\n",
    "\n",
    "        \n",
    "def specify_data(dataset, landmarks): \n",
    "    ds = np.concatenate([X_i for X_i in dataset])\n",
    "    \"\"\"\n",
    "    dataset should contain a 4D matrix of (5, _, 90, 126). \n",
    "    \"\"\" \n",
    "    cols = []\n",
    "    for landmark in landmarks:\n",
    "        cols += (np.array([0, 21, 42, 63, 84, 105]) + landmark).tolist()\n",
    "    \n",
    "    return ds[:, :, tuple(cols)]\n",
    "\n",
    "X_six = specify_data(X_all, [0, 4, 8, 12, 16, 20]).reshape(5, 20, 90, 36)\n",
    "X_one = specify_data(X_all, [0]).reshape(5, 20, 90, 6) \n",
    "\n",
    "import random \n",
    "def shuffle(X, y, seed = None):\n",
    "    if seed == None:  \n",
    "        seed = random.randrange(0, 100)\n",
    "        print(f\"using seed {seed}\")\n",
    "    np.random.seed(seed) \n",
    "    new_X = np.concatenate([X_i for X_i in X])\n",
    "    new_y = np.concatenate([y_i for y_i in y])\n",
    "    N = np.random.permutation(new_X.shape[0])\n",
    "    new_X = new_X[N]\n",
    "    new_y = new_y[N]\n",
    "    new_X = new_X.reshape(5, 20, 90, new_X.shape[-1])\n",
    "    new_y = new_y.reshape(5, 20)\n",
    "    return new_X, new_y"
   ],
   "outputs": [],
   "metadata": {
    "id": "75869bdd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "SEED = 65\n",
    "X_all, y = shuffle(X_all, y, seed = SEED)\n",
    "X_six, _ = shuffle(X_six, y, seed = SEED)\n",
    "X_six, _ = shuffle(X_six, y, seed = SEED)"
   ],
   "outputs": [],
   "metadata": {
    "id": "255dab3c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import sklearn.metrics\n",
    "def ensemble_val_acc(models, X_tests, y_test): \n",
    "    preds = np.zeros_like(y_test)\n",
    "    for model, X_test in zip(models, X_tests): \n",
    "        preds += model.predict(X_test).flatten() \n",
    "    preds = preds / len(models)\n",
    "    return (np.round_(preds) == y_test).mean(), sklearn.metrics.precision_score(y_test, np.round_(preds)), sklearn.metrics.recall_score(y_test, np.round_(preds))"
   ],
   "outputs": [],
   "metadata": {
    "id": "a66fe962"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import time, glob\n",
    "def cross_validate(make_model_one, make_model_six, X_one, X_six, y, epochs = 75, callbacks = [], verbose = 1): \n",
    "    val_accuracies, precisions, recalls = [], [], []\n",
    "    for i in range(5): \n",
    "        model_one = make_model_one() \n",
    "        model_six = make_model_six()\n",
    "        \n",
    "        # define global labels\n",
    "        y_train = np.concatenate([y_j for j, y_j in enumerate(y) if i != j])\n",
    "        y_test = y[i]\n",
    "        \n",
    "        # first run all \n",
    "        X_test_one = X_one[i]\n",
    "        X_train_one = np.concatenate([X_j for j, X_j in enumerate(X_one) if i != j])\n",
    "        \n",
    "        try:\n",
    "            os.remove(\"best_aso.h5\") \n",
    "        except Exception as e: \n",
    "            pass \n",
    "\n",
    "        # train \n",
    "        history_one = model_one.fit(X_train_one, y_train, validation_data = (X_test_one, y_test), epochs = epochs, callbacks = callbacks, verbose = verbose)\n",
    "        \n",
    "        try: \n",
    "            model_one.load_weights(\"best_aso.h5\")\n",
    "        except Exception as e: \n",
    "            pass\n",
    "        \n",
    "        print(\"\\nevaluation on all:\")\n",
    "        model_one.evaluate(X_test_one, y_test)\n",
    "        \n",
    "        time.sleep(1)\n",
    "        # next train six \n",
    "        \n",
    "        X_test_six = X_six[i]\n",
    "        X_train_six = np.concatenate([X_j for j, X_j in enumerate(X_six) if i != j])\n",
    "        \n",
    "        try:\n",
    "            os.remove(\"best_aso.h5\") \n",
    "        except Exception as e: \n",
    "            pass \n",
    "\n",
    "        # train \n",
    "        history_six = model_six.fit(X_train_six, y_train, validation_data = (X_test_six, y_test), epochs = epochs, callbacks = callbacks, verbose = verbose)\n",
    "        \n",
    "        try: \n",
    "            model_six.load_weights(\"best_aso.h5\")\n",
    "        except Exception as e: \n",
    "            pass\n",
    "        \n",
    "        print(\"\\nevaluation on six:\")\n",
    "        model_six.evaluate(X_test_six, y_test)\n",
    "        time.sleep(1)\n",
    "    \n",
    "        # YAY! WE HAVE TRAINED THE MODEL ON EVERYTHING FOR THIS FOLD\n",
    "    \n",
    "        # get the aggregate validation accuracy on everything\n",
    "        models = [model_one, model_six]\n",
    "        val_acc, pres, recall = ensemble_val_acc(models, [X_test_one, X_test_six], y_test)\n",
    "        val_accuracies.append(val_acc)\n",
    "        precisions.append(pres)\n",
    "        recalls.append(recall)\n",
    "        print(f\"ensemble validation accuracy : {(val_acc, pres, recall)}\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        for video in glob.glob(\"*mov\"): \n",
    "            print(f\"video is {video}, {predict_on_video(models, video)}\")\n",
    "    print(f\"average : {sum(val_accuracies) / len(val_accuracies), sum(precisions) / len(precisions), sum(recalls) / len(recalls)}\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "0390341a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# create the functions to create models \n",
    "import tensorflow as tf \n",
    "def make_model_one(): \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(32, return_sequences=False), \n",
    "        tf.keras.layers.Dropout(0.3), \n",
    "        tf.keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def make_model_six(): \n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(64, return_sequences=False, input_shape = (None, 36)), \n",
    "        tf.keras.layers.Dropout(0.3), \n",
    "        tf.keras.layers.Dense(1, activation = 'sigmoid') \n",
    "    ]) \n",
    "\n",
    "    model.compile(loss = \"binary_crossentropy\", optimizer = tf.keras.optimizers.Adam(learning_rate=0.01), metrics = ['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {
    "id": "7c02d84d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import mediapipe as mp \n",
    "from tqdm import tqdm \n",
    "def hand_locations_general(frame, min_detection_confidence = 0.5, min_tracking_confidence = 0.5): \n",
    "    \"\"\"give all landmarks\"\"\"\n",
    "\n",
    "    hands = mp.solutions.hands.Hands(min_detection_confidence=min_detection_confidence, min_tracking_confidence=min_tracking_confidence) # MAKE SURE THIS IS ALL GOOD \n",
    "    results = hands.process(frame.astype('uint8'))\n",
    "    X_locations = [0] * 42\n",
    "    Y_locations = [0] * 42\n",
    "    Z_locations = [0] * 42\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        x = y = z = 0 \n",
    "        for hand, hand_landmark in enumerate(results.multi_hand_landmarks):\n",
    "            for i in range(0, 21):\n",
    "                landmark = hand_landmark.landmark[i]\n",
    "                X_locations[x] = landmark.x\n",
    "                Y_locations[y] = landmark.y \n",
    "                Z_locations[z] = landmark.z\n",
    "                x += 1; y += 1; z +=1; \n",
    "            \n",
    "    hands.close()\n",
    "    return np.concatenate([X_locations, Y_locations, Z_locations]) \n",
    "\n",
    "\n",
    "# create a function to pad your videos \n",
    "def pad(locations, maxlen = 90, padding = \"post\", truncating = \"post\"): \n",
    "    new_locations = locations.tolist() \n",
    "    empty_row = np.zeros((1, 126))\n",
    "    for i, video in tqdm(enumerate(new_locations)): \n",
    "        if len(video) < maxlen:  \n",
    "            for new_row in range(maxlen - len(video)): \n",
    "                if padding == \"post\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([new_locations[i], empty_row])\n",
    "                if padding == \"pre\": \n",
    "                    new_locations[i] = np.array(new_locations[i])\n",
    "                    new_locations[i] = np.concatenate([empty_row, new_locations[i]])\n",
    "\n",
    "        if len(video) > maxlen: \n",
    "            if truncating == \"post\": \n",
    "                new_locations[i] = new_locations[i][:maxlen]\n",
    "            elif truncating == \"pre\": \n",
    "                new_locations[i] = new_locations[i][len(video) - maxlen : ]\n",
    "    return np.array(new_locations)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4da2ec83",
    "outputId": "23d4a2a5-a369-4966-b529-0fefac2595d2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "model_to_arrangements = {1 : [0], 2 : [0, 4, 8, 12, 16, 20]}"
   ],
   "outputs": [],
   "metadata": {
    "id": "4qDuUuABWpJr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# function to ensemble predict on frames. \n",
    "def ensemble_predict(models, X_test):\n",
    "    preds = np.zeros((X_test.shape[0], 1))\n",
    "    for model, landmarks in tqdm(zip(models, model_to_arrangements.values())):\n",
    "        test_data = specify_data(np.array([X_test]), landmarks)\n",
    "        preds += model.predict(test_data)\n",
    "    preds = preds / len(models)\n",
    "    return preds.flatten()"
   ],
   "outputs": [],
   "metadata": {
    "id": "ms0s-2miWlb0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# create a function to then predict on videos \n",
    "import cv2, numpy as np\n",
    "def predict_on_video(models, path): \n",
    "    LOCATIONS = []\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    while cap.isOpened():\n",
    "        _, frame = cap.read()\n",
    "        if not _: break \n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        LOCATIONS.append(hand_locations_general(frame))\n",
    "    \n",
    "    print('read all locations')\n",
    "    LOCATIONS = np.array([LOCATIONS])\n",
    "    LOCATIONS = pad(LOCATIONS)\n",
    "    print(\"padded\")\n",
    "    return ensemble_predict(models, LOCATIONS)"
   ],
   "outputs": [],
   "metadata": {
    "id": "4de0a3df"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"best_aso.h5\", save_best_only=True, monitor = \"val_accuracy\")\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor = \"val_accuracy\", patience=10)\n",
    "cross_validate(make_model_one, make_model_six, X_one, X_six, y, epochs = 75, callbacks=[checkpoint, early_stopping], verbose = 0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "evaluation on all:\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6570 - accuracy: 0.6500 - precision_2: 0.6667 - recall_2: 0.2500\n",
      "\n",
      "evaluation on six:\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8571 - accuracy: 0.4500 - precision_3: 0.3846 - recall_3: 0.6250\n",
      "ensemble validation accuracy : (0.5, 0.42857142857142855, 0.75)\n"
     ]
    }
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a19a5cd5",
    "outputId": "dee9a319-5d55-4c2e-f9a3-4ee41ad95b7f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# let's see whether this runs with jupyter now re-installed. "
   ],
   "outputs": [],
   "metadata": {
    "id": "472d64c2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "all_six_one_ensemble.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}